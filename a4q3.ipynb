{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# A4-Q3: Convolutional Autoencoders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preliminaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# In case you are fortunate enough to have access to a GPU...\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true
      },
      "source": [
        "# Dataset: MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "hidden": true
      },
      "outputs": [],
      "source": [
        "# You can change img_size to 14 if you want to use smaller (14x14) images.\n",
        "img_size = 28\n",
        "ds_full = torchvision.datasets.MNIST('./files/', train=True, download=True,\n",
        "                            transform=torchvision.transforms.Compose([\n",
        "                            torchvision.transforms.Resize((img_size,img_size)),\n",
        "                            torchvision.transforms.ToTensor(),\n",
        "                            ]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "hidden": true
      },
      "outputs": [],
      "source": [
        "ds = torch.utils.data.Subset(ds_full, range(1024))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "hidden": true
      },
      "outputs": [],
      "source": [
        "def Draw(x):\n",
        "    with torch.no_grad():\n",
        "        plt.imshow(x.squeeze().detach().numpy(), cmap='gray');\n",
        "        plt.axis('off');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "hidden": true
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    x,t = ds.__getitem__(130)\n",
        "    Draw(x)\n",
        "    plt.title(f'Size: {list(x.size())}')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true
      },
      "source": [
        "## Create some `DataLoader`s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "hidden": true
      },
      "outputs": [],
      "source": [
        "# Batched, for training\n",
        "batch_size_train = 8\n",
        "train_dl = torch.utils.data.DataLoader(ds, batch_size=batch_size_train, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "hidden": true
      },
      "outputs": [],
      "source": [
        "# A single batch, for plotting\n",
        "train_all = torch.utils.data.DataLoader(ds, batch_size=1024, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# A. Complete the `ConvAE` class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ConvAE(nn.Module):\n",
        "    '''\n",
        "     net = ConvAE(img_size=28, embedding_dim=3)\n",
        "     \n",
        "     Create a convolutional autoencoder for imput images of size (img_size x img_size),\n",
        "     with an embedding (latent) layer of (embedding_dim) neurons.\n",
        "     \n",
        "     Inputs:\n",
        "       img_size       size of input images, [1, img_size, img_size]\n",
        "       embedding_dim  number of nodes in embedding (latent) layer\n",
        "       \n",
        "     Usage:\n",
        "       net = ConvAE()\n",
        "       y = net(x)\n",
        "       h = net.encode(x)  # returns latent vectors\n",
        "    '''\n",
        "    def __init__(self, img_size=28, embedding_dim=3):\n",
        "        self.img_size = img_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.losses = []\n",
        "        super().__init__()\n",
        "        \n",
        "        #===== YOUR CODE HERE =====\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(True),\n",
        "            nn.Flatten()\n",
        "        )\n",
        "        self.fc_enc = nn.Sequential(\n",
        "            nn.Linear(64 * 4 * 4, 128),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(128, embedding_dim),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        self.fc_dec = nn.Sequential(\n",
        "            nn.Linear(embedding_dim, 128),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(128, 64 * 4 * 4),\n",
        "            nn.ReLU(True),\n",
        "            nn.Unflatten(1, (64, 4, 4))\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(16, 1, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        \n",
        "    def encode(self, x):\n",
        "        #===== YOUR CODE HERE =====\n",
        "        x = self.encoder(x)\n",
        "        h = self.fc_enc(x)\n",
        "        return h\n",
        "    \n",
        "    \n",
        "    def decode(self, x):\n",
        "        #===== YOUR CODE HERE =====\n",
        "        h = self.fc_dec(x)\n",
        "        x = self.decoder(h)\n",
        "        x = x[:, :, :self.img_size, :self.img_size]\n",
        "        return x\n",
        "  \n",
        "\n",
        "    def forward(self, x):\n",
        "        #===== YOUR CODE HERE =====\n",
        "        h = self.encode(x)\n",
        "        y = self.decode(h)\n",
        "        return y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# B. Create and train the network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "net = ConvAE(img_size=img_size, embedding_dim=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/20: 100%|██████████| 128/128 [00:00<00:00, 306.76it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/20]  Average Loss: 0.101784\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/20: 100%|██████████| 128/128 [00:00<00:00, 327.83it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/20]  Average Loss: 0.066434\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/20: 100%|██████████| 128/128 [00:00<00:00, 331.09it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3/20]  Average Loss: 0.066225\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/20: 100%|██████████| 128/128 [00:00<00:00, 312.95it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [4/20]  Average Loss: 0.066140\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/20: 100%|██████████| 128/128 [00:00<00:00, 297.68it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [5/20]  Average Loss: 0.066101\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/20: 100%|██████████| 128/128 [00:00<00:00, 329.79it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [6/20]  Average Loss: 0.066189\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/20: 100%|██████████| 128/128 [00:00<00:00, 342.96it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [7/20]  Average Loss: 0.066051\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/20: 100%|██████████| 128/128 [00:00<00:00, 309.16it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [8/20]  Average Loss: 0.066136\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9/20: 100%|██████████| 128/128 [00:00<00:00, 339.83it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [9/20]  Average Loss: 0.066094\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10/20: 100%|██████████| 128/128 [00:00<00:00, 329.11it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [10/20]  Average Loss: 0.066052\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 11/20: 100%|██████████| 128/128 [00:00<00:00, 339.20it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [11/20]  Average Loss: 0.066137\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 12/20: 100%|██████████| 128/128 [00:00<00:00, 337.90it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [12/20]  Average Loss: 0.066085\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 13/20: 100%|██████████| 128/128 [00:00<00:00, 340.72it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [13/20]  Average Loss: 0.066010\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 14/20: 100%|██████████| 128/128 [00:00<00:00, 333.79it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [14/20]  Average Loss: 0.066038\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 15/20: 100%|██████████| 128/128 [00:00<00:00, 336.09it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [15/20]  Average Loss: 0.066026\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 16/20: 100%|██████████| 128/128 [00:00<00:00, 337.47it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [16/20]  Average Loss: 0.065996\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 17/20: 100%|██████████| 128/128 [00:00<00:00, 324.03it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [17/20]  Average Loss: 0.065959\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 18/20: 100%|██████████| 128/128 [00:00<00:00, 319.02it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [18/20]  Average Loss: 0.066025\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 19/20: 100%|██████████| 128/128 [00:00<00:00, 309.23it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [19/20]  Average Loss: 0.065994\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 20/20: 100%|██████████| 128/128 [00:00<00:00, 336.43it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [20/20]  Average Loss: 0.066014\n",
            "\n",
            "Training complete!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Train it\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
        "criterion = nn.MSELoss()\n",
        "epochs = 20\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    net.train()\n",
        "    running_loss = 0\n",
        "\n",
        "    for x, _ in tqdm(train_dl, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
        "        x = x.to(device)\n",
        "\n",
        "        y_pred = net(x)\n",
        "\n",
        "        loss = criterion(y_pred, x)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    avg_loss = running_loss / len(train_dl)\n",
        "    net.losses.append(avg_loss)\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}]  Average Loss: {avg_loss:.6f}\")\n",
        "\n",
        "print(\"\\nTraining complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ConvAE(\n",
              "  (encoder): Sequential(\n",
              "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "    (5): ReLU(inplace=True)\n",
              "    (6): Flatten(start_dim=1, end_dim=-1)\n",
              "  )\n",
              "  (fc_enc): Sequential(\n",
              "    (0): Linear(in_features=1024, out_features=128, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Linear(in_features=128, out_features=3, bias=True)\n",
              "    (3): Tanh()\n",
              "  )\n",
              "  (fc_dec): Sequential(\n",
              "    (0): Linear(in_features=3, out_features=128, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Linear(in_features=128, out_features=1024, bias=True)\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): Unflatten(dim=1, unflattened_size=(64, 4, 4))\n",
              "  )\n",
              "  (decoder): Sequential(\n",
              "    (0): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): ConvTranspose2d(32, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): ConvTranspose2d(16, 1, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
              "    (5): Sigmoid()\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Uncomment the following line if you want to save your network.\n",
        "#torch.save(net.to('cpu'), 'my_ConvAE.pt')\n",
        "# The corresponding code to reload the network is below.\n",
        "#net = torch.load('my_ConvAE.pt')\n",
        "# Remember to send it to the GPU, if you're using one.\n",
        "net.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# C. Plot the latent space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Here are 10 colours you can use. But feel free to use others, too.\n",
        "colour_options = ['k', 'tab:brown', 'r', 'orange', 'gold', 'lawngreen', 'forestgreen', 'blue', 'mediumpurple', 'gray']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute the latent-space representation for all the samples.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# You can create three 2D planar projections.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Or you can plot a 3D scatter plot.\n",
        "fig = plt.figure(figsize=(7,6))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "# ax.scatter(...)\n",
        "#===== YOUR CODE HERE =====\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# D. Plot reconstructed digit images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Digit Reconstructions\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
