{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# A4-Q3: Convolutional Autoencoders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preliminaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# In case you are fortunate enough to have access to a GPU...\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true
      },
      "source": [
        "# Dataset: MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true
      },
      "outputs": [],
      "source": [
        "# You can change img_size to 14 if you want to use smaller (14x14) images.\n",
        "img_size = 28\n",
        "ds_full = torchvision.datasets.MNIST('./files/', train=True, download=True,\n",
        "                            transform=torchvision.transforms.Compose([\n",
        "                            torchvision.transforms.Resize((img_size,img_size)),\n",
        "                            torchvision.transforms.ToTensor(),\n",
        "                            ]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true
      },
      "outputs": [],
      "source": [
        "ds = torch.utils.data.Subset(ds_full, range(1024))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true
      },
      "outputs": [],
      "source": [
        "def Draw(x):\n",
        "    with torch.no_grad():\n",
        "        plt.imshow(x.squeeze().detach().numpy(), cmap='gray');\n",
        "        plt.axis('off');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    x,t = ds.__getitem__(130)\n",
        "    Draw(x)\n",
        "    plt.title(f'Size: {list(x.size())}')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true
      },
      "source": [
        "## Create some `DataLoader`s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true
      },
      "outputs": [],
      "source": [
        "# Batched, for training\n",
        "batch_size_train = 8\n",
        "train_dl = torch.utils.data.DataLoader(ds, batch_size=batch_size_train, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true
      },
      "outputs": [],
      "source": [
        "# A single batch, for plotting\n",
        "train_all = torch.utils.data.DataLoader(ds, batch_size=1024, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# A. Complete the `ConvAE` class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ConvAE(nn.Module):\n",
        "    '''\n",
        "     net = ConvAE(img_size=28, embedding_dim=3)\n",
        "     \n",
        "     Create a convolutional autoencoder for imput images of size (img_size x img_size),\n",
        "     with an embedding (latent) layer of (embedding_dim) neurons.\n",
        "     \n",
        "     Inputs:\n",
        "       img_size       size of input images, [1, img_size, img_size]\n",
        "       embedding_dim  number of nodes in embedding (latent) layer\n",
        "       \n",
        "     Usage:\n",
        "       net = ConvAE()\n",
        "       y = net(x)\n",
        "       h = net.encode(x)  # returns latent vectors\n",
        "    '''\n",
        "    def __init__(self, img_size=28, embedding_dim=3):\n",
        "        self.img_size = img_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.losses = []\n",
        "        super().__init__()\n",
        "        \n",
        "        #===== YOUR CODE HERE =====\n",
        "        \n",
        "        \n",
        "        \n",
        "    def encode(self, x):\n",
        "        #===== YOUR CODE HERE =====\n",
        "        return x  # replace this line\n",
        "    \n",
        "    \n",
        "    def decode(self, x):\n",
        "        #===== YOUR CODE HERE =====\n",
        "        return x  # replace this line\n",
        "  \n",
        "\n",
        "    def forward(self, x):\n",
        "        #===== YOUR CODE HERE =====\n",
        "        return x  # replace this line\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# B. Create and train the network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "net = ConvAE(img_size=img_size, embedding_dim=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train it\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment the following line if you want to save your network.\n",
        "#torch.save(net.to('cpu'), 'my_ConvAE.pt')\n",
        "# The corresponding code to reload the network is below.\n",
        "#net = torch.load('my_ConvAE.pt')\n",
        "# Remember to send it to the GPU, if you're using one.\n",
        "#net.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# C. Plot the latent space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Here are 10 colours you can use. But feel free to use others, too.\n",
        "colour_options = ['k', 'tab:brown', 'r', 'orange', 'gold', 'lawngreen', 'forestgreen', 'blue', 'mediumpurple', 'gray']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute the latent-space representation for all the samples.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# You can create three 2D planar projections.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Or you can plot a 3D scatter plot.\n",
        "fig = plt.figure(figsize=(7,6))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "# ax.scatter(...)\n",
        "#===== YOUR CODE HERE =====\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# D. Plot reconstructed digit images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Digit Reconstructions\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "zapata",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
